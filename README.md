# SLT Bug Analytics Dashboard

This repository contains a Streamlit dashboard for analysing bug reports in the SLT Selfcare app.

## Running Locally

Install dependencies and start Streamlit:

```bash
pip install -r requirements.txt
streamlit run src/dashboard/app.py
```

The application reads configuration from environment variables. For local
development you can place them in a `.env` file, while in GitHub Actions these
values come from repository secrets. The `OPENAI_API_KEY` is used by the data
processing scripts and `bug_email_notifier.py` requires email credentials. A minimal `.env` might look like:

```env
OPENAI_API_KEY=
# EMAIL_USER=you@example.com
# EMAIL_PASS=app_password
# EMAIL_TO=
```

## Deployment Notes

When deploying on platforms such as Streamlit Community Cloud or other headless environments, include the `.streamlit/config.toml` file in the repository. This file ensures the server runs in headless mode and disables CORS, which is required for remote access:

```toml
[server]
headless = true
enableCORS = false
```

Having these settings in place lets the dashboard start automatically without needing a browser on the server and prevents CORS issues when accessing the app.

## About This Project
This dashboard visualises and summarises bug reports collected from the SLT Selfcare app on Google Play. The data processing pipeline gathers reviews, classifies potential bugs using a transformer model and groups them into categories. Developer-friendly summaries are generated with OpenAI models.

## Data Pipeline
Running `python -m data_processing.run_pipeline` executes the end-to-end workflow:

1. `sltmobitel_app_review` scrapes the latest Play Store reviews.
2. `prioritized_bugs` identifies likely bug reports.
3. `bug__categories` performs an initial classification.
4. `reclassified_bugs_with_sbert` refines the categories with SBERT embeddings.
5. `bug__categories_v2` produces final analytics-ready CSV files.
6. `developer_summary` creates concise summaries for each bug category.

Each stage writes its output both as CSV and into a SQLite database at
`data/bug_data.db`. The dashboard will use the database if the CSV files are not
available. A scheduled GitHub Actions workflow called **Three Days Update**
executes the pipeline every day at 1:00&nbsp;AM UTC and commits the updated CSV
files and database to the repository.

## Dashboard Overview
Launching the app opens an interactive interface with filtering options, visualisations and developer insights. Non-bug feedback is analysed separately to highlight sentiment trends and common topics. Export buttons allow downloading filtered results and category summaries. A manual button triggers the `bug_email_notifier.py` script to send the latest digest via email.

## Repository Layout
- `src/data_processing/` – data pipeline modules
- `src/dashboard/` – Streamlit application and email notifier
- `data/` – CSV files and `bug_data.db` generated by the pipeline

With the necessary environment variables configured, running the pipeline then launching Streamlit provides a full end-to-end view of bug reports and user feedback.
